{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [Deequ](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here is a great blog post on Deequ for more information:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "\n",
    "![Deequ](img/deequ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.2)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==1.0.2) (2018.4)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==1.0.2) (1.14.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==1.0.2) (2.7.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.2) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Docker Container with Spark and our Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset preprocessing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM openjdk:8-jre-slim\r\n",
      "\r\n",
      "RUN apt-get update\r\n",
      "RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\r\n",
      "RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\r\n",
      "RUN apt-get clean\r\n",
      "RUN rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed\r\n",
      "ENV PYTHONHASHSEED 0\r\n",
      "ENV PYTHONIOENCODING UTF-8\r\n",
      "ENV PIP_DISABLE_PIP_VERSION_CHECK 1\r\n",
      "\r\n",
      "# Install Hadoop\r\n",
      "ENV HADOOP_VERSION 3.0.0\r\n",
      "ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\r\n",
      "ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n",
      "ENV PATH $PATH:$HADOOP_HOME/bin\r\n",
      "RUN curl -sL --retry 3 \\\r\n",
      "  \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\" \\\r\n",
      "  | gunzip \\\r\n",
      "  | tar -x -C /usr/ \\\r\n",
      " && rm -rf $HADOOP_HOME/share/doc \\\r\n",
      " && chown -R root:root $HADOOP_HOME\r\n",
      "\r\n",
      "# Install Spark\r\n",
      "ENV SPARK_VERSION 2.4.5\r\n",
      "ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\r\n",
      "ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\r\n",
      "ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\r\n",
      "ENV PATH $PATH:${SPARK_HOME}/bin\r\n",
      "RUN curl -sL --retry 3 \\\r\n",
      "  \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\" \\\r\n",
      "  | gunzip \\\r\n",
      "  | tar x -C /usr/ \\\r\n",
      " && mv /usr/$SPARK_PACKAGE $SPARK_HOME \\\r\n",
      " && chown -R root:root $SPARK_HOME\r\n",
      " \r\n",
      "# Point Spark at proper python binary\r\n",
      "ENV PYSPARK_PYTHON=/usr/bin/python3\r\n",
      "\r\n",
      "# Setup Spark/Yarn/HDFS user as root\r\n",
      "ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\r\n",
      "ENV YARN_RESOURCEMANAGER_USER=\"root\"\r\n",
      "ENV YARN_NODEMANAGER_USER=\"root\"\r\n",
      "ENV HDFS_NAMENODE_USER=\"root\"\r\n",
      "ENV HDFS_DATANODE_USER=\"root\"\r\n",
      "ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\r\n",
      "\r\n",
      "# Set up bootstrapping program and Spark configuration\r\n",
      "COPY program /opt/program\r\n",
      "RUN chmod +x /opt/program/submit\r\n",
      "COPY hadoop-config /opt/hadoop-config\r\n",
      "\r\n",
      "COPY jars /usr/jars\r\n",
      "\r\n",
      "WORKDIR $SPARK_HOME\r\n",
      "\r\n",
      "ENTRYPOINT [\"/opt/program/submit\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-analyzer'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  3.023MB\n",
      "Step 1/33 : FROM openjdk:8-jre-slim\n",
      " ---> 381b20190cf7\n",
      "Step 2/33 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> f23f81127dbe\n",
      "Step 3/33 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> fc24f6140cb8\n",
      "Step 4/33 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 886b8e409755\n",
      "Step 5/33 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 9821503e0874\n",
      "Step 6/33 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 8ffd96b022b0\n",
      "Step 7/33 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> b01bb045738d\n",
      "Step 8/33 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 46ce98ac38cc\n",
      "Step 9/33 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 53bc16742191\n",
      "Step 10/33 : ENV HADOOP_VERSION 3.0.0\n",
      " ---> Using cache\n",
      " ---> 41cc7d13d705\n",
      "Step 11/33 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> fc57049f703f\n",
      "Step 12/33 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 8181fe7f6c03\n",
      "Step 13/33 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> 6fcc9c65cebe\n",
      "Step 14/33 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> 0ed657e7981b\n",
      "Step 15/33 : ENV SPARK_VERSION 2.4.5\n",
      " ---> Using cache\n",
      " ---> 3390b5f831dd\n",
      "Step 16/33 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 7268a4152db3\n",
      "Step 17/33 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> ad3a601dab84\n",
      "Step 18/33 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> 18d052905fde\n",
      "Step 19/33 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> 8cae49f1213e\n",
      "Step 20/33 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> e76f46f3e932\n",
      "Step 21/33 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> b8e83a2b0069\n",
      "Step 22/33 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 56e0fa58d42b\n",
      "Step 23/33 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 47f97d7cca1e\n",
      "Step 24/33 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 1c9414c472af\n",
      "Step 25/33 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> a48358e2784c\n",
      "Step 26/33 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> f2c2c61c1204\n",
      "Step 27/33 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> aa6ebf7821be\n",
      "Step 28/33 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> 5582bb1462c8\n",
      "Step 29/33 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 24d628454998\n",
      "Step 30/33 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 7b2614ddad50\n",
      "Step 31/33 : COPY jars /usr/jars\n",
      " ---> Using cache\n",
      " ---> 07eb518b58d2\n",
      "Step 32/33 : WORKDIR $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> f6c0c4c8c59a\n",
      "Step 33/33 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Using cache\n",
      " ---> 1ee4eb257124\n",
      "Successfully built 1ee4eb257124\n",
      "Successfully tagged amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Spark container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478630443205.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"repositories\": [\r\n",
      "        {\r\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-west-2:478630443205:repository/amazon-reviews-spark-analyzer\",\r\n",
      "            \"registryId\": \"478630443205\",\r\n",
      "            \"repositoryName\": \"amazon-reviews-spark-analyzer\",\r\n",
      "            \"repositoryUri\": \"478630443205.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer\",\r\n",
      "            \"createdAt\": 1584820852.0,\r\n",
      "            \"imageTagMutability\": \"MUTABLE\",\r\n",
      "            \"imageScanningConfiguration\": {\r\n",
      "                \"scanOnPush\": false\r\n",
      "            }\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [478630443205.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer]\n",
      "\n",
      "\u001b[1B6ac90e38: Preparing \n",
      "\u001b[1Bf71688ae: Preparing \n",
      "\u001b[1B5e38ea34: Preparing \n",
      "\u001b[1B75a12f89: Preparing \n",
      "\u001b[1B5c4062f6: Preparing \n",
      "\u001b[1B35271017: Preparing \n",
      "\u001b[1Baef52e42: Preparing \n",
      "\u001b[1Bf78a1f3e: Preparing \n",
      "\u001b[1Bd32776fd: Preparing \n",
      "\u001b[1B0903d714: Preparing \n",
      "\u001b[1B461eb7a4: Preparing \n",
      "\u001b[1Bf99cd11c: Preparing \n",
      "\u001b[1B36ed7861: Preparing \n",
      "\u001b[1B8dafa5c7: Preparing \n",
      "\u001b[2B8dafa5c7: Layer already exists \u001b[9A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[Klatest: digest: sha256:8b6c064183742535142f4ed986e921ee7fe66dd4a42807ba736a9643c3691252 size: 3472\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run our Analysis Job as a SageMaker Processing Job\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "from __future__ import unicode_literals\r\n",
      "\r\n",
      "import time\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import shutil\r\n",
      "import csv\r\n",
      "\r\n",
      "import pyspark\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "from pyspark.sql.functions import *\r\n",
      "\r\n",
      "def main():\r\n",
      "    args_iter = iter(sys.argv[1:])\r\n",
      "    args = dict(zip(args_iter, args_iter))\r\n",
      "    \r\n",
      "    # Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\r\n",
      "    s3_input_data = args['s3_input_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_input_data)\r\n",
      "    s3_output_analyze_data = args['s3_output_analyze_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_output_analyze_data)\r\n",
      "    \r\n",
      "    spark = SparkSession.builder \\\r\n",
      "        .appName(\"Amazon_Reviews_Spark_Analyzer\") \\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    # Invoke Main from preprocess-deequ.jar\r\n",
      "    getattr(spark._jvm.SparkAmazonReviewsAnalyzer, \"run\")(s3_input_data, s3_output_analyze_data)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "cat preprocess-deequ.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.8xlarge',\n",
    "                            env={\n",
    "                                'mode': 'jar',\n",
    "                                'main_class': 'Main'\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-478630443205/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 17:47:23   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-03-21 17:47:27   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-03-21-20-13-07\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-478630443205/amazon-reviews-spark-analyzer-2020-03-21-20-13-07/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-478630443205/spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-478630443205/spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-deequ.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_analyze_data', s3_output_analyze_data,\n",
    "              ],\n",
    "              # See https://github.com/aws/sagemaker-python-sdk/issues/1341 for why we need to specify a dummy-output\n",
    "              outputs=[\n",
    "                  ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                   output_name='dummy-output',\n",
    "                                   source='/opt/ml/processing/output')\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, processing_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-478630443205/amazon-reviews-spark-analyzer-2020-03-21-20-13-07/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-478630443205/spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'dummy-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-478630443205/spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900/output/dummy-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.8xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '478630443205.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-analyzer:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-deequ.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-478630443205/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-west-2-478630443205/amazon-reviews-spark-analyzer-2020-03-21-20-13-07/output']}, 'Environment': {'main_class': 'Main', 'mode': 'jar'}, 'RoleArn': 'arn:aws:iam::478630443205:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:478630443205:processing-job/spark-amazon-reviews-analyzer-2020-03-21-20-13-07-900', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 3, 21, 20, 13, 8, 243000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 3, 21, 20, 13, 8, 243000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'aeb731b1-5d87-4995-9cc1-884da72cb44a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'aeb731b1-5d87-4995-9cc1-884da72cb44a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1628', 'date': 'Sat, 21 Mar 2020 20:13:08 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output (Quality Checks on our Dataset)\n",
    "Take a look at a few rows of the transformed dataset to make sure the preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 247515 does not meet the constraint req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MinimumConstraint(Minimum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>MaximumConstraint(Maximum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_id,...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(review_id)))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplac...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Error</td>\n",
       "      <td>Error</td>\n",
       "      <td>ComplianceConstraint(Compliance(marketplace co...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          check check_level check_status  \\\n",
       "0  Review Check       Error        Error   \n",
       "1  Review Check       Error        Error   \n",
       "2  Review Check       Error        Error   \n",
       "3  Review Check       Error        Error   \n",
       "4  Review Check       Error        Error   \n",
       "5  Review Check       Error        Error   \n",
       "6  Review Check       Error        Error   \n",
       "\n",
       "                                          constraint constraint_status  \\\n",
       "0                         SizeConstraint(Size(None))           Failure   \n",
       "1       MinimumConstraint(Minimum(star_rating,None))           Success   \n",
       "2       MaximumConstraint(Maximum(star_rating,None))           Success   \n",
       "3  CompletenessConstraint(Completeness(review_id,...           Success   \n",
       "4  UniquenessConstraint(Uniqueness(List(review_id)))           Success   \n",
       "5  CompletenessConstraint(Completeness(marketplac...           Success   \n",
       "6  ComplianceConstraint(Compliance(marketplace co...           Success   \n",
       "\n",
       "                                  constraint_message  \n",
       "0  Value: 247515 does not meet the constraint req...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  \n",
       "5                                                NaN  \n",
       "6                                                NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_checks = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-checks/', sep='\\t', header=0)\n",
    "df_constraint_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>ApproxCountDistinct</td>\n",
       "      <td>238027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,star_rating</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>-0.080881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Mean</td>\n",
       "      <td>3.723706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>top star_rating</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>0.663338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,helpful_votes</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>0.980529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                   instance                 name          value\n",
       "0       Column                  review_id         Completeness       1.000000\n",
       "1       Column                  review_id  ApproxCountDistinct  238027.000000\n",
       "2  Mutlicolumn    total_votes,star_rating          Correlation      -0.080881\n",
       "3      Dataset                          *                 Size  247515.000000\n",
       "4       Column                star_rating                 Mean       3.723706\n",
       "5       Column            top star_rating           Compliance       0.663338\n",
       "6  Mutlicolumn  total_votes,helpful_votes          Correlation       0.980529"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/dataset-metrics/', sep='\\t', header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Uniqueness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace contained in US,UK,DE,JP,FR</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity                                 instance          name     value\n",
       "0   Column                                review_id  Completeness       1.0\n",
       "1   Column                                review_id    Uniqueness       1.0\n",
       "2  Dataset                                        *          Size  247515.0\n",
       "3   Column                              star_rating       Maximum       5.0\n",
       "4   Column                              star_rating       Minimum       1.0\n",
       "5   Column  marketplace contained in US,UK,DE,JP,FR    Compliance       1.0\n",
       "6   Column                              marketplace  Completeness       1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_success_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/success-metrics/', sep='\\t', header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_1</th>\n",
       "      <th>_2</th>\n",
       "      <th>_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review_id</td>\n",
       "      <td>'review_id' is not null</td>\n",
       "      <td>.isComplete(\\review_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' is not null</td>\n",
       "      <td>.isComplete(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has type Integral</td>\n",
       "      <td>.hasDataType(\\customer_id\\\", ConstrainableData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has no negative values</td>\n",
       "      <td>.isNonNegative(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>review_date</td>\n",
       "      <td>'review_date' is not null</td>\n",
       "      <td>.isComplete(\\review_date\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' is not null</td>\n",
       "      <td>.isComplete(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' is not null</td>\n",
       "      <td>.isComplete(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' has no negative values</td>\n",
       "      <td>.isNonNegative(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>product_title</td>\n",
       "      <td>'product_title' is not null</td>\n",
       "      <td>.isComplete(\\product_title\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_headline</td>\n",
       "      <td>'review_headline' is not null</td>\n",
       "      <td>.isComplete(\\review_headline\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>product_id</td>\n",
       "      <td>'product_id' is not null</td>\n",
       "      <td>.isComplete(\\product_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' is not null</td>\n",
       "      <td>.isComplete(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' is not null</td>\n",
       "      <td>.isComplete(\\product_category\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' has value range 'Digital_Vi...</td>\n",
       "      <td>.isContainedIn(\\product_category\\\", Array(\\\"Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' is not null</td>\n",
       "      <td>.isComplete(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has type Integral</td>\n",
       "      <td>.hasDataType(\\product_parent\\\", ConstrainableD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has no negative values</td>\n",
       "      <td>.isNonNegative(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_body</td>\n",
       "      <td>'review_body' has less than 1% missing values</td>\n",
       "      <td>.hasCompleteness(\\review_body\\\", _ &gt;= 0.99, So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' is not null</td>\n",
       "      <td>.isComplete(\\vine\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' has value range 'N'</td>\n",
       "      <td>.isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' is not null</td>\n",
       "      <td>.isComplete(\\marketplace\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' has value range 'US'</td>\n",
       "      <td>.isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' is not null</td>\n",
       "      <td>.isComplete(\\verified_purchase\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' has value range 'Y', 'N'</td>\n",
       "      <td>.isContainedIn(\\verified_purchase\\\", Array(\\\"Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   _1                                                 _2  \\\n",
       "0           review_id                            'review_id' is not null   \n",
       "1         customer_id                          'customer_id' is not null   \n",
       "2         customer_id                    'customer_id' has type Integral   \n",
       "3         customer_id               'customer_id' has no negative values   \n",
       "4         review_date                          'review_date' is not null   \n",
       "5       helpful_votes                        'helpful_votes' is not null   \n",
       "6       helpful_votes             'helpful_votes' has no negative values   \n",
       "7         star_rating                          'star_rating' is not null   \n",
       "8         star_rating               'star_rating' has no negative values   \n",
       "9       product_title                        'product_title' is not null   \n",
       "10    review_headline                      'review_headline' is not null   \n",
       "11         product_id                           'product_id' is not null   \n",
       "12        total_votes                          'total_votes' is not null   \n",
       "13        total_votes               'total_votes' has no negative values   \n",
       "14   product_category                     'product_category' is not null   \n",
       "15   product_category  'product_category' has value range 'Digital_Vi...   \n",
       "16     product_parent                       'product_parent' is not null   \n",
       "17     product_parent                 'product_parent' has type Integral   \n",
       "18     product_parent            'product_parent' has no negative values   \n",
       "19        review_body      'review_body' has less than 1% missing values   \n",
       "20               vine                                 'vine' is not null   \n",
       "21               vine                         'vine' has value range 'N'   \n",
       "22        marketplace                          'marketplace' is not null   \n",
       "23        marketplace                 'marketplace' has value range 'US'   \n",
       "24  verified_purchase                    'verified_purchase' is not null   \n",
       "25  verified_purchase       'verified_purchase' has value range 'Y', 'N'   \n",
       "\n",
       "                                                   _3  \n",
       "0                          .isComplete(\\review_id\\\")\"  \n",
       "1                        .isComplete(\\customer_id\\\")\"  \n",
       "2   .hasDataType(\\customer_id\\\", ConstrainableData...  \n",
       "3                     .isNonNegative(\\customer_id\\\")\"  \n",
       "4                        .isComplete(\\review_date\\\")\"  \n",
       "5                      .isComplete(\\helpful_votes\\\")\"  \n",
       "6                   .isNonNegative(\\helpful_votes\\\")\"  \n",
       "7                        .isComplete(\\star_rating\\\")\"  \n",
       "8                     .isNonNegative(\\star_rating\\\")\"  \n",
       "9                      .isComplete(\\product_title\\\")\"  \n",
       "10                   .isComplete(\\review_headline\\\")\"  \n",
       "11                        .isComplete(\\product_id\\\")\"  \n",
       "12                       .isComplete(\\total_votes\\\")\"  \n",
       "13                    .isNonNegative(\\total_votes\\\")\"  \n",
       "14                  .isComplete(\\product_category\\\")\"  \n",
       "15  .isContainedIn(\\product_category\\\", Array(\\\"Di...  \n",
       "16                    .isComplete(\\product_parent\\\")\"  \n",
       "17  .hasDataType(\\product_parent\\\", ConstrainableD...  \n",
       "18                 .isNonNegative(\\product_parent\\\")\"  \n",
       "19  .hasCompleteness(\\review_body\\\", _ >= 0.99, So...  \n",
       "20                              .isComplete(\\vine\\\")\"  \n",
       "21             .isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"  \n",
       "22                       .isComplete(\\marketplace\\\")\"  \n",
       "23     .isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"  \n",
       "24                 .isComplete(\\verified_purchase\\\")\"  \n",
       "25  .isContainedIn(\\verified_purchase\\\", Array(\\\"Y...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
